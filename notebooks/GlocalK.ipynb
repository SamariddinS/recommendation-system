{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "sys.path.append('/home/samariddin/projects/recommender-system/')\n",
    "# Get the absolute path of the current working directory\n",
    "abs_path = os.path.abspath('.')\n",
    "\n",
    "path_to_data = os.path.join('/home', 'samariddin', 'projects', 'recommender-system', 'data', 'dataset', 'ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import src.metrics as ml_metrics\n",
    "from src.recommender.recommender import GlocalK\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_categorical(df_X, _X):\n",
    "\tvalues = np.array(df_X[_X])\n",
    "\t# integer encode\n",
    "\tlabel_encoder = LabelEncoder()\n",
    "\tinteger_encoded = label_encoder.fit_transform(values)\n",
    "\t# binary encode\n",
    "\tonehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\tinteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "\tonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\tdf_X = df_X.drop(columns=_X)\n",
    "\tfor j in range(integer_encoded.max() + 1):\n",
    "\t\tdf_X.insert(loc=j + 1, column=str(_X) + str(j + 1), value=onehot_encoded[:, j])\n",
    "\treturn df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(df, df_user, alpha_coefs=[0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045], alpha_param=1682):\n",
    "\tfor alpha_coef in alpha_coefs:\n",
    "\t\tpairs = []\n",
    "\t\tgrouped = df.groupby(['MID', 'rate'])\n",
    "\n",
    "\t\tfor key, group in grouped:\n",
    "\t\t\tpairs.extend(list(combinations(group['UID'], 2)))\n",
    "\n",
    "\t\tcounter = Counter(pairs)\n",
    "\t\talpha = alpha_coef * alpha_param  # 1m = 3883, param*i_no\n",
    "\t\tedge_list = map(\n",
    "\t\t\tlist,\n",
    "\t\t\tCounter(el for el in counter.elements()\n",
    "\t\t\t\t\tif counter[el] >= alpha).keys())\n",
    "\t\tG = nx.Graph()\n",
    "\n",
    "\t\tfor el in edge_list:\n",
    "\t\t\tG.add_edge(el[0], el[1], weight=1)\n",
    "\t\t\tG.add_edge(el[0], el[0], weight=1)\n",
    "\t\t\tG.add_edge(el[1], el[1], weight=1)\n",
    "\n",
    "\t\tpr = nx.pagerank(G.to_directed())\n",
    "\t\tdf_user['PR'] = df_user['UID'].map(pr)\n",
    "\t\tdf_user['PR'] /= float(df_user['PR'].max())\n",
    "\t\tdc = nx.degree_centrality(G)\n",
    "\t\tdf_user['CD'] = df_user['UID'].map(dc)\n",
    "\t\tdf_user['CD'] /= float(df_user['CD'].max())\n",
    "\t\tcc = nx.closeness_centrality(G)\n",
    "\t\tdf_user['CC'] = df_user['UID'].map(cc)\n",
    "\t\tdf_user['CC'] /= float(df_user['CC'].max())\n",
    "\t\tbc = nx.betweenness_centrality(G)\n",
    "\t\tdf_user['CB'] = df_user['UID'].map(bc)\n",
    "\t\tdf_user['CB'] /= float(df_user['CB'].max())\n",
    "\t\tlc = nx.load_centrality(G)\n",
    "\t\tdf_user['LC'] = df_user['UID'].map(lc)\n",
    "\t\tdf_user['LC'] /= float(df_user['LC'].max())\n",
    "\t\tnd = nx.average_neighbor_degree(G, weight='weight')\n",
    "\t\tdf_user['AND'] = df_user['UID'].map(nd)\n",
    "\t\tdf_user['AND'] /= float(df_user['AND'].max())\n",
    "\t\tX_train = df_user.loc[:, df_user.columns[1:]]\n",
    "\t\tX_train.fillna(0, inplace=True)\n",
    "\n",
    "\t\tX_train.to_pickle(\"/home/samariddin/projects/recommender-system/data/extracted_features/features_alpha(\" + str(alpha_coef) +\").pkl\")\n",
    "\n",
    "\treturn X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path=\"/home/samariddin/projects/recommender-system/data/dataset/\", ratings_data=\"ml-100k/u1.base\", users_data=\"ml-100k/u.user\", test_data=\"ml-100k/u1.test\", rating_sep='\\t', users_sep='\\\\|'):\n",
    "\tratings = pd.read_csv(data_path +\"/\"+ratings_data,\n",
    "\t\t\t\t\t\tsep=rating_sep,\n",
    "\t\t\t\t\t\tengine='python',\n",
    "\t\t\t\t\t\tnames=['UID', 'MID', 'rate', 'time'])\n",
    "\tdf_user = pd.read_csv(data_path +\"/\"+users_data,\n",
    "\t\t\t\t\t\tsep=users_sep,\n",
    "\t\t\t\t\t\tengine='python',\n",
    "\t\t\t\t\t\tnames=['UID', 'age', 'gender', 'job', 'zip'])\n",
    "\ttrain = np.loadtxt(data_path +'/'+ ratings_data, skiprows=0, delimiter=rating_sep).astype(\"int32\")\n",
    "\ttest = np.loadtxt(data_path + '/'+ test_data, skiprows=0, delimiter=rating_sep).astype(\"int32\")\n",
    "\ttotal = np.concatenate((train, test), axis=0)\n",
    "\n",
    "\t# User Features\n",
    "\tdf_user = convert_categorical(df_user, 'job')\n",
    "\tdf_user = convert_categorical(df_user, 'gender')\n",
    "\tdf_user['bin'] = pd.cut(df_user['age'], [0, 10, 20, 30, 40, 50, 100],\n",
    "\t\t\t\t\t\t\tlabels=['1', '2', '3', '4', '5', '6'])\n",
    "\tdf_user['age'] = df_user['bin']\n",
    "\n",
    "\tdf_user = df_user.drop(columns='bin')\n",
    "\tdf_user = convert_categorical(df_user, 'age')\n",
    "\tdf_user = df_user.drop(columns='zip')\n",
    "\n",
    "\tX_train = extract(ratings, df_user)\n",
    "\t# - - - - - - - - -\n",
    "\n",
    "\t# Prepar data\n",
    "\tn_u = np.unique(total[:, 0]).size  # num of users\n",
    "\tn_m = np.unique(total[:, 1]).size  # num of movies\n",
    "\tn_train = train.shape[0]  # num of training ratings\n",
    "\tn_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "\ttrain_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "\ttest_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "\n",
    "\tfor i in range(n_train):\n",
    "\t\ttrain_r[train[i, 1] - 1, train[i, 0] - 1] = train[i, 2]\n",
    "\n",
    "\tfor i in range(n_test):\n",
    "\t\ttest_r[test[i, 1] - 1, test[i, 0] - 1] = test[i, 2]\n",
    "\n",
    "\ttrain_m = np.greater(train_r, 1e-12).astype(\"float32\")  # masks indicating non-zero entries\n",
    "\n",
    "\t# Append the movies in X_train to the end of the existing movies in train_r\n",
    "\ttrain_r = np.concatenate((train_r,  X_train.T), axis=0).astype('float32')\n",
    "\n",
    "\t# save the ndarray object to a file using pickle\n",
    "\twith open(\"/home/samariddin/projects/recommender-system/data/train_data/train_r_(\" + str(n_u) +\").pkl\", \"wb\") as f:\n",
    "\t\tpickle.dump(train_r, f)\n",
    "\n",
    "\treturn n_m, n_u, train_r, train_m, test_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_1m(path=path_to_data):\n",
    "    \n",
    "    ratings = pd.read_csv(path+'/ratings.dat', sep='::', header=None, \n",
    "                          names=['user_id', 'item_id', 'rating', 'timestamp'], engine='python')\n",
    "    users = pd.read_csv(path+'/users.dat', sep='::', header=None,\n",
    "                        names=['user_id', 'gender', 'age', 'occupation', 'zip'], engine='python')\n",
    "\n",
    "    n_u = users.shape[0]  # num of users\n",
    "    n_ratings = ratings.shape[0]  # num of ratings\n",
    "\n",
    "    max_movie_id = 3952  # maximum movie ID\n",
    "    \n",
    "    train_r = np.zeros((max_movie_id, n_u), dtype='float32')\n",
    "    test_r = np.zeros((max_movie_id, n_u), dtype='float32')\n",
    "\n",
    "    # Splitting the ratings into training and test sets\n",
    "    train_size = int(0.8 * n_ratings)\n",
    "    test_size = n_ratings - train_size\n",
    "    shuffled_idx = np.random.permutation(n_ratings)\n",
    "\n",
    "    train_idx = shuffled_idx[:train_size]\n",
    "    test_idx = shuffled_idx[train_size:]\n",
    "\n",
    "    train_ratings = ratings.iloc[train_idx]\n",
    "    test_ratings = ratings.iloc[test_idx]\n",
    "\n",
    "    for i, row in train_ratings.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        if item_id <= max_movie_id:\n",
    "            train_r[item_id-1, row['user_id']-1] = row['rating']\n",
    "\n",
    "    for i, row in test_ratings.iterrows():\n",
    "        item_id = row['item_id']\n",
    "        if item_id <= max_movie_id:\n",
    "            test_r[item_id-1, row['user_id']-1] = row['rating']\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    return max_movie_id, n_u, train_r, train_m, test_r, test_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_m, n_u, train_r, train_m, test_r = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 35)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load features\n",
    "dataPath = \"/home/samariddin/projects/recommender-system/data/\"\n",
    "# X_train = pd.read_pickle(dataPath + \"extracted_features/features_alpha(0.045).pkl\").values.astype(float)\n",
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE-TRAINING finished.\n",
      "FINE-TUNING finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epochs_p': 103,\n",
       " 'epochs_f': 134,\n",
       " 'best_rmse_p': 0.9673729,\n",
       " 'best_rmse_f': 0.95729446}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate and train the model\n",
    "recommender = GlocalK()\n",
    "metrics = recommender.fit(train_r)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend for all users\n",
    "res = recommender.predict(np.arange(n_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recommendations(user_id: int, n: int):\n",
    "\tpredicted_ratings = res.T[user_id - 1]  # user_id starts from 1\n",
    "\trated_movies = np.where(train_m[:, user_id - 1] > 0)[0]  # movies already rated by user\n",
    "\tunrated_movies = np.setdiff1d(np.arange(n_m), rated_movies)  # movies not rated by user\n",
    "\tpredicted_ratings[rated_movies] = -np.inf  # set rated movies' rating to -inf, so they won't be recommended\n",
    "\n",
    "\t# get top-n recommended movie IDs\n",
    "\ttop_n = predicted_ratings.argsort()[::-1][:n]\n",
    "\ttop_n_movie_ids = [movie_id + 1 for movie_id in top_n if movie_id in unrated_movies]\n",
    "\n",
    "\treturn top_n_movie_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[170, 134, 357, 483, 479, 647, 478, 318, 603, 427]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 1\n",
    "topN = get_top_n_recommendations(user_id, 10)\n",
    "topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[170, 134, 357, 483, 479, 647, 478, 318, 603, 427]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = 1\n",
    "topN = get_top_n_recommendations(user_id, 10)\n",
    "topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (random):\t 0.0020718804433759665 \n",
      "GlocalK:\t\t 0.014454699631168999\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the recommendations\n",
    "k=50\n",
    "ground_truth = np.argsort(-test_r, axis=0)[:k,:].T.tolist()\n",
    "recommended = np.argsort(-res, axis=0)[:k,:].T.tolist()\n",
    "random = np.random.randint(0,n_m,(n_u, k)).T.tolist()\n",
    "\n",
    "print(\"Baseline (random):\\t\", ml_metrics.mapk(ground_truth, random, k=k), \"\\nGlocalK:\\t\\t\", ml_metrics.mapk(ground_truth, recommended, k=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoded features to file\n",
    "encoded_features_df = pd.DataFrame(res)\n",
    "encoded_features_df.to_pickle(dataPath + 'recommendations/recommendations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the u.item file\n",
    "item_df = pd.read_csv(dataPath+'dataset/ml-100k/u.item', sep='|', encoding='latin-1', header=None, usecols=[0,1], names=['MID', 'name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cinema Paradiso (1988)\n",
      "Citizen Kane (1941)\n",
      "One Flew Over the Cuckoo's Nest (1975)\n",
      "Casablanca (1942)\n",
      "Vertigo (1958)\n",
      "Ran (1985)\n",
      "Philadelphia Story, The (1940)\n",
      "Schindler's List (1993)\n",
      "Rear Window (1954)\n",
      "To Kill a Mockingbird (1962)\n"
     ]
    }
   ],
   "source": [
    "# Recommendations top-n\n",
    "for mid in topN:\n",
    "    movie_name = item_df.loc[item_df['MID'] == mid]['name'].values[0]\n",
    "    print(movie_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ratings data\n",
    "ratings_d = pd.read_csv(dataPath+\"dataset/ml-100k/u1.base\", delimiter='\\t', names=['UID', 'MID', 'rate', 'timestamp'], usecols=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Story (1995)\n",
      "Amadeus (1984)\n",
      "Jean de Florette (1986)\n",
      "Manon of the Spring (Manon des sources) (1986)\n",
      "Monty Python and the Holy Grail (1974)\n",
      "Wrong Trousers, The (1993)\n",
      "Empire Strikes Back, The (1980)\n",
      "Princess Bride, The (1987)\n",
      "Aliens (1986)\n",
      "12 Angry Men (1957)\n",
      "Return of the Jedi (1983)\n",
      "Terminator, The (1984)\n",
      "Dead Man Walking (1995)\n",
      "Graduate, The (1967)\n",
      "Nikita (La Femme Nikita) (1990)\n",
      "Back to the Future (1985)\n",
      "Cyrano de Bergerac (1990)\n",
      "When Harry Met Sally... (1989)\n",
      "Sling Blade (1996)\n",
      "Chasing Amy (1997)\n",
      "Chasing Amy (1997)\n",
      "Full Monty, The (1997)\n",
      "Sleeper (1973)\n",
      "Big Night (1996)\n",
      "Godfather, The (1972)\n",
      "Lone Star (1996)\n",
      "Mighty Aphrodite (1995)\n",
      "Mr. Holland's Opus (1995)\n",
      "French Twist (Gazon maudit) (1995)\n",
      "Antonia's Line (1995)\n",
      "Crumb (1994)\n",
      "Clerks (1994)\n",
      "Eat Drink Man Woman (1994)\n",
      "Hoop Dreams (1994)\n",
      "Star Wars (1977)\n",
      "Professional, The (1994)\n",
      "Priest (1994)\n",
      "Three Colors: Red (1994)\n",
      "Searching for Bobby Fischer (1993)\n",
      "Blade Runner (1982)\n",
      "Welcome to the Dollhouse (1995)\n",
      "Mystery Science Theater 3000: The Movie (1996)\n",
      "Truth About Cats & Dogs, The (1996)\n",
      "Haunted World of Edward D. Wood Jr., The (1995)\n",
      "Maya Lin: A Strong Clear Vision (1994)\n",
      "Gattaca (1997)\n"
     ]
    }
   ],
   "source": [
    "# Movies user rated about 4-5\n",
    "ratings_d[\"UID\"] = ratings_d[\"UID\"].astype(int)\n",
    "ratings_d[\"rate\"] = ratings_d[\"rate\"].astype(int)\n",
    "ratings_d = ratings_d[ratings_d[\"rate\"] > 4]\n",
    "ratings_d = ratings_d[ratings_d[\"UID\"] == user_id]\n",
    "ratings_d = ratings_d.sort_values(by=\"rate\", ascending=False)\n",
    "ratings_d = ratings_d.drop_duplicates(subset=[\"MID\"], keep=\"first\")\n",
    "ratings_d = ratings_d.drop([\"UID\", \"rate\"], axis=1)\n",
    "\n",
    "for mid in ratings_d[\"MID\"]:\n",
    "    movie_name = item_df.loc[item_df[\"MID\"] == mid][\"name\"].values[0]\n",
    "    print(movie_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('.venv-notebook': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "751992c4ce1f56fa93ec1d9fc5e055a0179dac987b115d18254ab1731ca49b77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
